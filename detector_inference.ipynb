{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.6.0+cu92 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 0.4.1, 0.4.1.post2, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch==1.6.0+cu92\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.7.0+cu92 (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.5.0, 0.6.0, 0.6.1, 0.7.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torchvision==0.7.0+cu92\u001b[0m\n",
      "Requirement already up-to-date: timm in /opt/conda/lib/python3.7/site-packages (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.19.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (7.2.0)\n",
      "Requirement already satisfied, skipping upgrade: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (0.18.2)\n",
      "Requirement already satisfied: dlib==19.17.0 in /opt/conda/lib/python3.7/site-packages (19.17.0)\n",
      "Requirement already up-to-date: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (4.4.0.42)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-contrib-python) (1.19.1)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (4.4.0.42)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python) (1.19.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall torch torchvision\n",
    "#!pip install -U torch torchvision\n",
    "!pip install -U torch==1.6.0+cu92\n",
    "!pip install -U torchvision==0.7.0+cu92\n",
    "#!conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=9.2 -c pytorch\n",
    "#!pip install -U torchvision==0.5.0\n",
    "!pip install -U timm\n",
    "!pip install dlib==19.17.0\n",
    "!pip install -U opencv-contrib-python\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet_pytorch in /opt/conda/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from facenet_pytorch) (6.2.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from facenet_pytorch) (1.17.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from facenet_pytorch) (2.22.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from facenet_pytorch) (0.7.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->facenet_pytorch) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->facenet_pytorch) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->facenet_pytorch) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->facenet_pytorch) (3.0.4)\n",
      "Requirement already satisfied: torch==1.6.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->facenet_pytorch) (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.6.0->torchvision->facenet_pytorch) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "#import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0+cu92\n",
      "0.7.0+cu92\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2, os, dlib\n",
    "from facenet_pytorch import MTCNN\n",
    "from os.path import join\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from py_utils.face_utils import lib\n",
    "from py_utils.vid_utils import proc_vid as pv\n",
    "from py_utils.DL.sppnet.models.classifier import SPPNet\n",
    "from py_utils.DL.efficientnet.models.classifiers import build_model\n",
    "from py_utils.DL.efficientnet.models.cv_util import isotropically_resize_image\n",
    "from py_utils.DL.efficientnet.models.cv_util import padding_image\n",
    "from py_utils.DL.xceptionnet.models import model_selection   ## for Xception Network\n",
    "from skimage import io\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import re\n",
    "\n",
    "# CAM\n",
    "from interpretability.grad_cam import GradCAM, GradCamPlusPlus\n",
    "from interpretability.guided_back_propagation import GuidedBackPropagation\n",
    "from interpretability.utils import (get_last_conv_name, prepare_input,\n",
    "                                    gen_cam, norm_image, gen_gb, save_image)\n",
    "\n",
    "import logging\n",
    "\n",
    "LOGGING_FORMAT = '%(asctime)s %(levelname)s: %(message)s'\n",
    "DATE_FORMAT = '%Y%m%d %H:%M:%S'\n",
    "logging.basicConfig(level=logging.ERROR, format=LOGGING_FORMAT, datefmt=DATE_FORMAT)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_test(front_face_detector, lmark_predictor, sample_num, net, im, input_size, cuda):\n",
    "\n",
    "    # print('begin im_test.....')\n",
    "    face_info = lib.align(im[:, :, (2,1,0)], front_face_detector, lmark_predictor)\n",
    "    # print('face_info finished.....')\n",
    "    # Samples\n",
    "    if len(face_info) != 1:\n",
    "        prob = -1\n",
    "    else:\n",
    "        _, point = face_info[0]\n",
    "        rois = []\n",
    "        for i in range(sample_num):\n",
    "            roi, _ = lib.cut_head([im], point, i)\n",
    "            rois.append(cv2.resize(roi[0], (input_size, input_size)))\n",
    "\n",
    "        # vis_ = np.concatenate(rois, 1)\n",
    "        # cv2.imwrite('vis.jpg', vis_)\n",
    "\n",
    "        bgr_mean = np.array([103.939, 116.779, 123.68])\n",
    "        bgr_mean = bgr_mean[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "        bgr_mean = torch.from_numpy(bgr_mean).float()\n",
    "        if cuda:\n",
    "            bgr_mean = bgr_mean.cuda()\n",
    "        \n",
    "        rois = torch.from_numpy(np.array(rois)).float()\n",
    "        if cuda:\n",
    "            rois = rois.cuda()\n",
    "        rois = rois.permute((0, 3, 1, 2))   ## 將tensor的維度進行轉換, 此列為置換維度順序\n",
    "        prob = net(rois - bgr_mean)\n",
    "        #prob = F.softmax(prob, dim=1)\n",
    "        prob = torch.sigmoid(prob)\n",
    "        prob = prob.data.cpu().numpy()\n",
    "        prob = 1 - np.mean(np.sort(prob[:, 0])[np.round(sample_num / 2).astype(int):])\n",
    "        \n",
    "        ##\n",
    "        _, points = np.array(face_info[0], dtype=object)\n",
    "        xmin = np.min(points[:, 0])\n",
    "        xmax = np.max(points[:, 0])\n",
    "        ymin = np.min(points[:, 1])\n",
    "        ymax = np.max(points[:, 1])\n",
    "        face = [xmin, ymin, xmax, ymax]\n",
    "        logger.info(f'im_test:{face}')\n",
    "    return prob, face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_mtcnn(fid, model_name, net, im, input_size, cuda):\n",
    "    device =  torch.device('cuda:0' if cuda else 'cpu')\n",
    "\n",
    "    face_detector = MTCNN(margin=0,thresholds=[0.85, 0.95, 0.95], device=device)    \n",
    "    \n",
    "    im_h, im_w, _ = im.shape\n",
    "    logger.info(f'im.shape:h-->{im_h}, w-->{im_w}')\n",
    "    try:\n",
    "        try:\n",
    "            face_info, _ = face_detector.detect(im, landmarks=False)\n",
    "            #face_info, _, _ = mtcnn.detect(im, landmarks=True)\n",
    "            logger.info(f'face_info:{face_info}')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"mtcnn error: {e}\")\n",
    "        \n",
    "        if face_info is not None:\n",
    "            xmin, ymin, xmax, ymax = [int(b) for b in face_info[0]]\n",
    "            logger.info(f'xmin:{xmin}, ymin:{ymin}, xmax:{xmax}, ymax:{ymax}')\n",
    "\n",
    "            # facenet detector\n",
    "            w = xmax - xmin\n",
    "            h = ymax - ymin\n",
    "            p_h = h // 3\n",
    "            p_w = w // 3\n",
    "            y1 = max(ymin - p_h, 0)\n",
    "            y2 = ymax + p_h\n",
    "            x1 = max(xmin - p_w, 0)\n",
    "            x2 = xmax + p_w\n",
    "            #logger.info(f'(y1:{y1}:y2:{y2},x1:{x1}:x2:{x2})')\n",
    "            inputs = im[y1:y2, x1:x2]\n",
    "            inputs_shape = inputs.shape\n",
    "            #logger.info(f'inputs_shape:{inputs_shape}')\n",
    "\n",
    "            #face = torch.from_numpy(np.array(face)).float()\n",
    "\n",
    "    #         # from Owen\n",
    "            INPUT_SIZE = 380\n",
    "    #         inputs = isotropically_resize_image(inputs, INPUT_SIZE)\n",
    "    #         inputs = padding_image(inputs, INPUT_SIZE)\n",
    "    #         cv2.imwrite(f'./output/inputs_{fid}.jpg', inputs)\n",
    "    #         inputs = inputs / 255.\n",
    "\n",
    "            face_h, face_w, _ = inputs.shape\n",
    "            #logger.info(f'face shape:{inputs.shape}')\n",
    "\n",
    "            # prepare for cam\n",
    "            face = inputs.copy()\n",
    "\n",
    "            # inference\n",
    "            inputs = inputs[np.newaxis, :, :, :]\n",
    "            #logger.info(f'inputs:{inputs.shape}')\n",
    "            inputs = torch.from_numpy(inputs).float()\n",
    "            inputs = inputs.permute((0, 3, 1, 2))   ## 將tensor的維度進行轉換, 此列為置換維度順序\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = net(inputs)\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            #logger.info(f'outputs:{outputs}')\n",
    "            del outputs\n",
    "            prob = probs[0][0]\n",
    "            #logger.info(f'prob:{prob}')\n",
    "\n",
    "\n",
    "\n",
    "            # grad_cam\n",
    "            mask_2 = np.zeros(im.shape[:2], np.uint8)\n",
    "            ## 製作白色遮罩\n",
    "            mask_2[y1:y2, x1:x2] = 255\n",
    "            ## 黑白相反\n",
    "            mask_inv = cv2.bitwise_not(mask_2)\n",
    "            masked_img_2 = cv2.bitwise_and(im, im, mask=mask_inv)        \n",
    "            #logger.info(f'mask:{masked_img_2.shape}')\n",
    "\n",
    "\n",
    "            # im_size = 224\n",
    "\n",
    "            for class_idx in [0]:\n",
    "                cam = grad_cam(fid, 'GradCAMpp', net, face, INPUT_SIZE, class_idx, cuda)\n",
    "                cam_h, cam_w, _ = cam.shape\n",
    "\n",
    "            ## 平移\n",
    "            tx, ty = x1, y1\n",
    "            M1 = np.float32([[1, 0, tx],   # 向右 tx\n",
    "                             [0, 1, ty]])  # 向下 ty\n",
    "            shift_img1 = cv2.warpAffine(cam, M1, (im_w, im_h))  #\n",
    "\n",
    "            cam_merge = masked_img_2 + shift_img1\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(cam_merge, f'Model:{model_name}', (50, 50), font, 1, (0, 255, 255), 3, cv2.LINE_AA)\n",
    "#             cv2.imwrite(f'./output/cam_merge:{fid}.jpg', cam_merge)\n",
    "\n",
    "            ## face\n",
    "            face = [x1, y1, x2, y2]\n",
    "            #logger.info(f'face_mtcnn:{face}')\n",
    "        else:\n",
    "            prob = None\n",
    "            face = None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"face_mtcnn error: {e}\")\n",
    "    \n",
    "    if cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "    return prob, face, cam_merge if face_info is not None else im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_face_score(model_name, im, face_info, prob, threshold):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    if face_info is not None:\n",
    "        x1, y1, x2, y2 = [int(b) for b in face_info]\n",
    "        # Fake: (0, 255, 0), Real: (0, 0, 255)\n",
    "        if prob >= threshold:\n",
    "            label = 'fake'\n",
    "            color = (0, 0, 255)\n",
    "        else:\n",
    "            label = 'real'\n",
    "            color = (0, 255, 0)\n",
    "        #label = 'fake' if prob >= 0.5 else 'real'\n",
    "        #color = (0, (1 - prob) * 255, prob * 255)\n",
    "        cv2.rectangle(im, (x1, y1), (x2, y2), color, 10)\n",
    "        cv2.putText(im, f'{prob:.3f}=>{label}', (x1, y2 + 50), font, 1, color, 3, cv2.LINE_AA)\n",
    "    cv2.putText(im, f'Model:{model_name}', (50, 50), font, 1, (0, 255, 255), 3, cv2.LINE_AA)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_xception(model_path, cuda=True):\n",
    "    # Load network\n",
    "    net = model_selection(modelname='xception', num_out_classes=2, dropout=0.5)\n",
    "    # load training weight\n",
    "    if cuda:\n",
    "        net.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        net.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "    if isinstance(net, torch.nn.DataParallel):\n",
    "        net = net.module\n",
    "    if cuda:\n",
    "        net = net.cuda()\n",
    "    # logger.info(f'loaded Xception model {net}')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_sppnet(model_path, cuda=True):\n",
    "    try:\n",
    "        # load network\n",
    "        net = SPPNet(backbone=50, num_class=2)\n",
    "        if cuda:\n",
    "            net = net.cuda()\n",
    "        net.eval()\n",
    "    except Exception as e:\n",
    "        logger.info(f'load network:{e}')\n",
    "    try:\n",
    "        # load training weight\n",
    "        if cuda:\n",
    "            checkpoint = torch.load(model_path)\n",
    "        else:\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        logger.info(checkpoint.keys())\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "    except Exception as e:\n",
    "        logger.info(f\"load weight:{e}\")\n",
    "    # logger.info(f'loaded SPPNet model {net}')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_efficientnet(model_path, cuda=True):\n",
    "    try:\n",
    "        # load network\n",
    "        # net = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\")\n",
    "        net, epoch, bce_best = build_model(encoder=\"tf_efficientnet_b7_ns\", weights=model_path, no_spp=False)\n",
    "        net.module.encoder.eval()\n",
    "        for p in net.module.encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "#         if cuda:\n",
    "#             net = net.cuda()\n",
    "#         net.eval()\n",
    "    except Exception as e:\n",
    "        logger.info(f'load network:{e}')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(fid, cam_model, net, im, im_size, class_idx=None, cuda=False):\n",
    "    \n",
    "    im_shape = im.shape\n",
    "    #logger.info(f'grad_cam im_shape:{im_shape}')\n",
    "    #img = np.float32(cv2.resize(im, (im_size, im_size))) / 255\n",
    "    img = np.float32(im)/255\n",
    "    inputs = prepare_input(img)\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    #logger.info(f'grad_cam inputs:{type(inputs)}')\n",
    "\n",
    "    # 输出图像\n",
    "    image_dict = {}\n",
    "    \n",
    "    layer_name = get_last_conv_name(net)\n",
    "    #logger.info(f'grad_cam layer_name:{layer_name}')\n",
    "\n",
    "\n",
    "    # Grad-CAM\n",
    "    if cam_model == 'GradCAM':\n",
    "        grad_cam = GradCAM(net, layer_name)\n",
    "        mask = grad_cam(inputs, class_idx)  # cam mask\n",
    "        # mask = cv2.resize(mask, (im.shape[1], im.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "        image_dict['GradCAM'], image_dict['heatmap'] = gen_cam(img, mask)\n",
    "        grad_cam.remove_handlers()\n",
    "    # Grad-CAM++\n",
    "    if cam_model == 'GradCAMpp':\n",
    "        grad_cam_plus_plus = GradCamPlusPlus(net, layer_name)\n",
    "        mask_plus_plus = grad_cam_plus_plus(inputs, class_idx)  # cam mask\n",
    "        image_dict['GradCAMpp'], image_dict['heatmap++'] = gen_cam(img, mask_plus_plus)\n",
    "#         cv2.imwrite(f'./output/GradCAMpp:{fid}.jpg', image_dict['GradCAMpp'])\n",
    "#         cv2.imwrite(f'./output/heatmappp:{fid}.jpg', image_dict['heatmap++'])\n",
    "        grad_cam_plus_plus.remove_handlers()\n",
    "    return image_dict[cam_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(start_frame, end_frame, num):\n",
    "    try:\n",
    "        # num = 30\n",
    "        sample_list = [random.randint(start_frame, end_frame) for _ in range(num)]\n",
    "        sample_list.extend([0])\n",
    "        sample_list = sorted(set(sample_list))\n",
    "        logger.info(f'sample_frames:{sample_list}')\n",
    "    except Exception as e:\n",
    "        logger.info(f'sample_frames:{e}')\n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detector_inference(model_name, video_path, model_path, output_path, threshold, cam=False,\n",
    "                       start_frame=0, end_frame=None, cuda=False):\n",
    "    logger.info('Starting: {}'.format(video_path))\n",
    "\n",
    "    cam_model = 'GradCAMpp'\n",
    "\n",
    "    video_fn = video_path.split('/')[-1].split('.')[0]\n",
    "    video_fn_cam = f'{video_fn}_{model_name}_{cam_model}.mp4'\n",
    "    video_fn = f'{video_fn}_{model_name}.mp4'\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    input_size = 224\n",
    "    class_idx = 0\n",
    "    \n",
    "    if model_name == 'SPPNet':\n",
    "        net = load_network_sppnet(model_path, cuda)\n",
    "    if model_name == 'XceptionNet':\n",
    "        net = load_network_xception(model_path, cuda)\n",
    "    if model_name == 'EfficientnetB7':\n",
    "        net = load_network_efficientnet(model_path, cuda)\n",
    "    \n",
    "    # mp4 file path\n",
    "    imgs, num_frames, fps, width, height = pv.parse_vid(video_path)\n",
    "    probs = []\n",
    "    frame = 0\n",
    "    # reader = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    writer = None\n",
    "    writer_cam = None\n",
    "    writer = cv2.VideoWriter(join(output_path, video_fn), fourcc, fps,\n",
    "                             (height, width)[::-1])\n",
    "    writer_cam = cv2.VideoWriter(join(output_path, video_fn_cam), fourcc, fps,\n",
    "                                (height, width)[::-1])\n",
    "\n",
    "\n",
    "    logger.info(f'num_frames:{num_frames}, fps:{fps}, width:{width}, height:{height}')\n",
    "    # Frame numbers and length of output video\n",
    "    frame_num = 0\n",
    "    assert start_frame < num_frames - 1\n",
    "    end_frame = end_frame if end_frame else num_frames\n",
    "\n",
    "    try:        \n",
    "        sample_list = sample_frames(start_frame, end_frame, 30)\n",
    "        pbar = tqdm(total=end_frame - start_frame)\n",
    "        for fid, im in enumerate(imgs):\n",
    "            pbar.update(1)\n",
    "\n",
    "            if fid in sample_list:\n",
    "                prob, face_info, cam_im = face_mtcnn(fid, model_name, net, im, input_size, cuda)\n",
    "                bnd_im = draw_face_score(model_name, im, face_info, prob, threshold)\n",
    "            else:\n",
    "                bnd_im = draw_face_score(model_name, im, face_info, prob, threshold)\n",
    "            writer.write(bnd_im)\n",
    "            writer_cam.write(cam_im)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f'generate image:{e}')\n",
    "    pbar.close()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "        logger.info(f'Finished! Output saved under {output_path}{video_fn}')\n",
    "    else:\n",
    "        logger.info('Input video file was empty')\n",
    "    if writer_cam is not None:\n",
    "        writer_cam.release()\n",
    "        logger.info(f'Finished! Grad-cam Output saved under {output_path}{video_fn_cam}')\n",
    "    else:\n",
    "        logger.info('Input video file was empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dropout 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3481e1f5a8894c518debdd4cbdb0ccbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=523.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'XceptionNet'\n",
    "model_path = './pretrained_model/df_c0_best.pkl'\n",
    "\n",
    "#video_path = './input/data_dst.mp4'\n",
    "#video_path = './input/Trump_AndyLiu_2.mp4'\n",
    "# video_path = './input/ff_real_yt_c40_999.mp4'\n",
    "#video_path = './input/ff_real_yt_c40_900.mp4'\n",
    "#video_path = './input/ff_real_yt_c40_901.mp4'\n",
    "#video_path = './input/ff_real_yt_c40_902.mp4'\n",
    "#video_path = './input/ff_fake_nt_c40_900_926.mp4'\n",
    "#video_path = './input/ff_fake_nt_c40_901_902.mp4'\n",
    "#video_path = './input/ff_fake_nt_c40_902_901.mp4'\n",
    "## c23\n",
    "#video_path = './input/ff_fake_df_c23_900_926.mp4'\n",
    "video_path = './input/ff_fake_df_c23_901_902.mp4'\n",
    "#video_path = './input/ff_fake_df_c23_902_901.mp4'\n",
    "#video_path = './input/ff_real_yt_c23_900.mp4'\n",
    "#video_path = './input/ff_real_yt_c23_901.mp4'\n",
    "#video_path = './input/ff_real_yt_c23_902.mp4'\n",
    "\n",
    "# FaceLab\n",
    "#video_path = './input/result_Andy_AIA.mp4'\n",
    "#video_path = './input/result_Andy_CSC.mp4'\n",
    "#video_path = './input/result_Andy_KP.mp4'\n",
    "#video_path = './input/result_Andy_Trump.mp4'\n",
    "#video_path = './input/data_dst_test_AIA.mp4'\n",
    "#video_path = './input/data_dst_test_CSC.mp4'\n",
    "#video_path = './input/data_dst_test_KP.mp4'\n",
    "\n",
    "\n",
    "output_path = './output/'\n",
    "threshold = 0.5\n",
    "cam = True\n",
    "cuda = True\n",
    "start_frame = 0\n",
    "end_frame = None\n",
    "\n",
    "detector_inference(model_name, video_path, model_path, output_path, threshold, cam, start_frame, end_frame, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3439432fe4d46d5bb145184fe9ed58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=523.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'SPPNet'\n",
    "model_path = './pretrained_model/SPP-res50.pth'\n",
    "\n",
    "#video_path = './input/data_dst.mp4'\n",
    "#video_path = './input/Trump_AndyLiu_2.mp4'\n",
    "# video_path = './input/ff_real_yt_c40_999.mp4'\n",
    "#video_path = './input/ff_real_yt_c40_900.mp4'\n",
    "#video_path = './input/ff_real_yt_c40_901.mp4'\n",
    "#video_path = './input/ff_real_yt_c40_902.mp4'\n",
    "#video_path = './input/ff_fake_nt_c40_900_926.mp4'\n",
    "#video_path = './input/ff_fake_nt_c40_901_902.mp4'\n",
    "#video_path = './input/ff_fake_nt_c40_902_901.mp4'\n",
    "## c23\n",
    "#video_path = './input/ff_fake_df_c23_900_926.mp4'\n",
    "video_path = './input/ff_fake_df_c23_901_902.mp4'\n",
    "#video_path = './input/ff_fake_df_c23_902_901.mp4'\n",
    "#video_path = './input/ff_real_yt_c23_900.mp4'\n",
    "#video_path = './input/ff_real_yt_c23_901.mp4'\n",
    "#video_path = './input/ff_real_yt_c23_902.mp4'\n",
    "\n",
    "# FaceLab\n",
    "#video_path = './input/result_Andy_AIA.mp4'\n",
    "#video_path = './input/result_Andy_CSC.mp4'\n",
    "#video_path = './input/result_Andy_KP.mp4'\n",
    "#video_path = './input/result_Andy_Trump.mp4'\n",
    "#video_path = './input/data_dst_test_AIA.mp4'\n",
    "#video_path = './input/data_dst_test_CSC.mp4'\n",
    "#video_path = './input/data_dst_test_KP.mp4'\n",
    "\n",
    "\n",
    "output_path = './output/'\n",
    "threshold = 0.5\n",
    "cam = False\n",
    "cuda = True\n",
    "start_frame = 0\n",
    "end_frame = None\n",
    "\n",
    "detector_inference(model_name, video_path, model_path, output_path, threshold, cam, start_frame, end_frame, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building DeepFakeSppClassifier...\n",
      "Loading checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
      "Loaded checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last' with epoch: 28, bce_best: 0.1639973577717397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ce8742a1ba487096a3a02109a9cb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=523.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EfficientnetB7'\n",
    "#model_path = './pretrained_model/tf_efficientnet_b7_ns_spp_last-Copy1'\n",
    "model_path = './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
    "\n",
    "#video_path = './input/data_dst.mp4'\n",
    "#video_path = './input/Trump_AndyLiu_2.mp4'\n",
    "# video_path = './input/ff_real_yt_c40_999.mp4'\n",
    "#video_path = './input/ff_real_yt_c40_900.mp4'\n",
    "#video_path = './input/ff_real_yt_c40_901.mp4'\n",
    "#video_path = './input/ff_real_yt_c40_902.mp4'\n",
    "#video_path = './input/ff_fake_nt_c40_900_926.mp4'\n",
    "#video_path = './input/ff_fake_nt_c40_901_902.mp4'\n",
    "#video_path = './input/ff_fake_nt_c40_902_901.mp4'\n",
    "## c23\n",
    "#video_path = './input/ff_fake_df_c23_900_926.mp4'\n",
    "video_path = './input/ff_fake_df_c23_901_902.mp4'\n",
    "#video_path = './input/ff_fake_df_c23_902_901.mp4'\n",
    "#video_path = './input/ff_real_yt_c23_900.mp4'\n",
    "#video_path = './input/ff_real_yt_c23_901.mp4'\n",
    "#video_path = './input/ff_real_yt_c23_902.mp4'\n",
    "\n",
    "# FaceLab\n",
    "#video_path = './input/result_Andy_AIA.mp4'\n",
    "#video_path = './input/result_Andy_CSC.mp4'\n",
    "#video_path = './input/result_Andy_KP.mp4'\n",
    "#video_path = './input/result_Andy_Trump.mp4'\n",
    "#video_path = './input/data_dst_test_AIA.mp4'\n",
    "#video_path = './input/data_dst_test_CSC.mp4'\n",
    "#video_path = './input/data_dst_test_KP.mp4'\n",
    "\n",
    "\n",
    "output_path = './output/'\n",
    "threshold = 0.5\n",
    "cam = True\n",
    "cuda = True\n",
    "start_frame = 0\n",
    "end_frame = None\n",
    "\n",
    "detector_inference(model_name, video_path, model_path, output_path, threshold, cam, start_frame, end_frame, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './input/'\n",
    "videos = [\n",
    "#     'data_dst.mp4',\n",
    "#     'Trump_AndyLiu_2.mp4',\n",
    "#     'ff_real_yt_c40_999.mp4',\n",
    "#     'ff_real_yt_c40_900.mp4',\n",
    "#     'ff_real_yt_c40_901.mp4',\n",
    "#     'ff_real_yt_c40_902.mp4',\n",
    "#     'ff_fake_nt_c40_900_926.mp4',\n",
    "#     'ff_fake_nt_c40_901_902.mp4',\n",
    "#     'ff_fake_nt_c40_902_901.mp4',\n",
    "#     'ff_fake_df_c23_900_926.mp4',\n",
    "#     'ff_fake_df_c23_901_902.mp4',\n",
    "#     'ff_fake_df_c23_902_901.mp4',\n",
    "#     'ff_real_yt_c23_900.mp4',\n",
    "#     'ff_real_yt_c23_901.mp4',\n",
    "    'ff_real_yt_c23_902.mp4',\n",
    "    'result_Andy_AIA.mp4',\n",
    "    'result_Andy_CSC.mp4',\n",
    "    'result_Andy_KP.mp4',\n",
    "    'result_Andy_Trump.mp4',\n",
    "    'data_dst_test_AIA.mp4',\n",
    "    'data_dst_test_CSC.mp4',\n",
    "    'data_dst_test_KP.mp4'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'XceptionNet' : './pretrained_model/df_c0_best.pkl',\n",
    "    'SPPNet' : './pretrained_model/SPP-res50.pth',\n",
    "    'EfficientnetB7' : './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './output/'\n",
    "threshold = 0.5\n",
    "cam = True\n",
    "cuda = True\n",
    "start_frame = 0\n",
    "end_frame = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dropout 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec8c8075d0a48d9a7ee275979002b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12bad9c3c9c4dc1a3118d0e900f231e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building DeepFakeSppClassifier...\n",
      "Loading checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
      "Loaded checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last' with epoch: 28, bce_best: 0.1639973577717397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644e2cdfe3cd470db69f5ded921edff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using dropout 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce835f7df69643f49aad46bb78171d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=456.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b548fe8422f448e39ac5ec00da4ac397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=456.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building DeepFakeSppClassifier...\n",
      "Loading checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
      "Loaded checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last' with epoch: 28, bce_best: 0.1639973577717397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e340f6a7ff64a8face08d43d411fd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=456.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using dropout 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3449e9cf9b4ac6ae075091a9aabaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f6fc8cc6724edcb6aec22ee26254a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building DeepFakeSppClassifier...\n",
      "Loading checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
      "Loaded checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last' with epoch: 28, bce_best: 0.1639973577717397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9b8f0e689b409d808709b40094218e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using dropout 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba62b2f2854a41feb1792a43aa5fef6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a72f41de5d4a83b2ebf136f893ae6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building DeepFakeSppClassifier...\n",
      "Loading checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
      "Loaded checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last' with epoch: 28, bce_best: 0.1639973577717397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c10c9bb5884a53b9b8cb69a17c0cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using dropout 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a291162b17c444d3bcd2a7c387dae637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=452.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d0d4fee05a4af78d1d05bc26d1c936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=452.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building DeepFakeSppClassifier...\n",
      "Loading checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
      "Loaded checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last' with epoch: 28, bce_best: 0.1639973577717397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e54acb89bd0446daf32524a2e94f4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=452.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using dropout 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c05806c75e84b24b1d9758ae0f5d315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=456.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a844313ed548bea72ffa9fe931545c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=456.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building DeepFakeSppClassifier...\n",
      "Loading checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
      "Loaded checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last' with epoch: 28, bce_best: 0.1639973577717397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840c7d5d67a44e6d871c4125787fe5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=456.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using dropout 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c10c8d46b94301b95f3942265581f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15761801bc2641fd856a8932a1363cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building DeepFakeSppClassifier...\n",
      "Loading checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
      "Loaded checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last' with epoch: 28, bce_best: 0.1639973577717397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad69b3797ee41708e72719c81407d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using dropout 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5beefa41f36848b69b794fc895760367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e328b6a2a844dcab9c258ac647d57e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building DeepFakeSppClassifier...\n",
      "Loading checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last'\n",
      "Loaded checkpoint './pretrained_model/tf_efficientnet_b7_ns_spp_last' with epoch: 28, bce_best: 0.1639973577717397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dcf25eb7e2436681658a197c0e7054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for video in videos:\n",
    "    video_path = join(path, video)\n",
    "    logger.info(f'video:{video}')\n",
    "    for model_name,model_path in model.items():\n",
    "        logger.info(f'model_name:{model_name}')\n",
    "        detector_inference(model_name, video_path, model_path, output_path, threshold, cam, start_frame, end_frame, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
